<html><head><title>WWW::RobotRules</title>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1" >
<link rel="stylesheet" title="blkbluw" type="text/css" href="../_blkbluw.css" media="all" >
<link rel="alternate stylesheet" title="blkmagw" type="text/css" href="../_blkmagw.css" media="all" >
<link rel="alternate stylesheet" title="blkcynw" type="text/css" href="../_blkcynw.css" media="all" >
<link rel="alternate stylesheet" title="whtprpk" type="text/css" href="../_whtprpk.css" media="all" >
<link rel="alternate stylesheet" title="whtnavk" type="text/css" href="../_whtnavk.css" media="all" >
<link rel="alternate stylesheet" title="grygrnk" type="text/css" href="../_grygrnk.css" media="all" >
<link rel="alternate stylesheet" title="whtgrng" type="text/css" href="../_whtgrng.css" media="all" >
<link rel="alternate stylesheet" title="blkgrng" type="text/css" href="../_blkgrng.css" media="all" >
<link rel="alternate stylesheet" title="grygrnw" type="text/css" href="../_grygrnw.css" media="all" >
<link rel="alternate stylesheet" title="blkbluw" type="text/css" href="../_blkbluw.css" media="all" >
<link rel="alternate stylesheet" title="whtpurk" type="text/css" href="../_whtpurk.css" media="all" >
<link rel="alternate stylesheet" title="whtgrng" type="text/css" href="../_whtgrng.css" media="all" >
<link rel="alternate stylesheet" title="grygrnw" type="text/css" href="../_grygrnw.css" media="all" >

<script type="text/javascript" src="../_podly.js"></script>

</head>
<body class='pod'>
<!--
  generated by Pod::Simple::HTML v3.16,
  using Pod::Simple::PullParser v3.16,
  under Perl v5.014002 at Tue Mar 25 17:14:28 2014 GMT.

 If you want to change this HTML document, you probably shouldn't do that
   by changing it directly.  Instead, see about changing the calling options
   to Pod::Simple::HTML, and/or subclassing Pod::Simple::HTML,
   then reconverting this document from the Pod source.
   When in doubt, email the author of Pod::Simple::HTML for advice.
   See 'perldoc Pod::Simple::HTML' for more info.

-->

<!-- start doc -->
<p class="backlinktop"><b><a name="___top" href="../index.html" accesskey="1" title="All Documents">&lt;&lt;</a></b></p>

<div class='indexgroup'>
<ul   class='indexList indexList1'>
  <li class='indexItem indexItem1'><a href='#NAME'>NAME</a>
  <li class='indexItem indexItem1'><a href='#SYNOPSIS'>SYNOPSIS</a>
  <li class='indexItem indexItem1'><a href='#DESCRIPTION'>DESCRIPTION</a>
  <li class='indexItem indexItem1'><a href='#ROBOTS.TXT'>ROBOTS.TXT</a>
  <li class='indexItem indexItem1'><a href='#ROBOTS.TXT_EXAMPLES'>ROBOTS.TXT EXAMPLES</a>
  <li class='indexItem indexItem1'><a href='#SEE_ALSO'>SEE ALSO</a>
  <li class='indexItem indexItem1'><a href='#COPYRIGHT'>COPYRIGHT</a>
</ul>
</div>

<h1><a class='u' href='#___top' title='click to go to top of document'
name="NAME"
>NAME</a></h1>

<p>WWW::RobotRules - database of robots.txt-derived permissions</p>

<h1><a class='u' href='#___top' title='click to go to top of document'
name="SYNOPSIS"
>SYNOPSIS</a></h1>

<pre> use WWW::RobotRules;
 my $rules = WWW::RobotRules-&#62;new(&#39;MOMspider/1.0&#39;);

 use LWP::Simple qw(get);

 {
   my $url = &#34;http://some.place/robots.txt&#34;;
   my $robots_txt = get $url;
   $rules-&#62;parse($url, $robots_txt) if defined $robots_txt;
 }

 {
   my $url = &#34;http://some.other.place/robots.txt&#34;;
   my $robots_txt = get $url;
   $rules-&#62;parse($url, $robots_txt) if defined $robots_txt;
 }

 # Now we can check if a URL is valid for those servers
 # whose &#34;robots.txt&#34; files we&#39;ve gotten and parsed:
 if($rules-&#62;allowed($url)) {
     $c = get $url;
     ...
 }</pre>

<h1><a class='u' href='#___top' title='click to go to top of document'
name="DESCRIPTION"
>DESCRIPTION</a></h1>

<p>This module parses <em>/robots.txt</em> files as specified in &#34;A Standard for Robot Exclusion&#34;, at &#60;http://www.robotstxt.org/wc/norobots.html&#62; Webmasters can use the <em>/robots.txt</em> file to forbid conforming robots from accessing parts of their web site.</p>

<p>The parsed files are kept in a WWW::RobotRules object, and this object provides methods to check if access to a given URL is prohibited. The same WWW::RobotRules object can be used for one or more parsed <em>/robots.txt</em> files on any number of hosts.</p>

<p>The following methods are provided:</p>

<dl>
<dt><a name="$rules_=_WWW::RobotRules-&#62;new($robot_name)"
>$rules = WWW::RobotRules-&#62;new($robot_name)</a></dt>

<dd>
<p>This is the constructor for WWW::RobotRules objects. The first argument given to new() is the name of the robot.</p>

<dt><a name="$rules-&#62;parse($robot_txt_url,_$content,_$fresh_until)"
>$rules-&#62;parse($robot_txt_url, $content, $fresh_until)</a></dt>

<dd>
<p>The parse() method takes as arguments the URL that was used to retrieve the <em>/robots.txt</em> file, and the contents of the file.</p>

<dt><a name="$rules-&#62;allowed($uri)"
>$rules-&#62;allowed($uri)</a></dt>

<dd>
<p>Returns TRUE if this robot is allowed to retrieve this URL.</p>

<dt><a name="$rules-&#62;agent([$name])"
>$rules-&#62;agent([$name])</a></dt>

<dd>
<p>Get/set the agent name. NOTE: Changing the agent name will clear the robots.txt rules and expire times out of the cache.</p>
</dd>
</dl>

<h1><a class='u' href='#___top' title='click to go to top of document'
name="ROBOTS.TXT"
>ROBOTS.TXT</a></h1>

<p>The format and semantics of the &#34;/robots.txt&#34; file are as follows (this is an edited abstract of &#60;http://www.robotstxt.org/wc/norobots.html&#62;):</p>

<p>The file consists of one or more records separated by one or more blank lines. Each record contains lines of the form</p>

<pre>  &#60;field-name&#62;: &#60;value&#62;</pre>

<p>The field name is case insensitive. Text after the &#39;#&#39; character on a line is ignored during parsing. This is used for comments. The following &#60;field-names&#62; can be used:</p>

<dl>
<dt><a name="User-Agent"
>User-Agent</a></dt>

<dd>
<p>The value of this field is the name of the robot the record is describing access policy for. If more than one <i>User-Agent</i> field is present the record describes an identical access policy for more than one robot. At least one field needs to be present per record. If the value is &#39;*&#39;, the record describes the default access policy for any robot that has not not matched any of the other records.</p>

<p>The <i>User-Agent</i> fields must occur before the <i>Disallow</i> fields. If a record contains a <i>User-Agent</i> field after a <i>Disallow</i> field, that constitutes a malformed record. This parser will assume that a blank line should have been placed before that <i>User-Agent</i> field, and will break the record into two. All the fields before the <i>User-Agent</i> field will constitute a record, and the <i>User-Agent</i> field will be the first field in a new record.</p>

<dt><a name="Disallow"
>Disallow</a></dt>

<dd>
<p>The value of this field specifies a partial URL that is not to be visited. This can be a full path, or a partial path; any URL that starts with this value will not be retrieved</p>
</dd>
</dl>

<p>Unrecognized records are ignored.</p>

<h1><a class='u' href='#___top' title='click to go to top of document'
name="ROBOTS.TXT_EXAMPLES"
>ROBOTS.TXT EXAMPLES</a></h1>

<p>The following example &#34;/robots.txt&#34; file specifies that no robots should visit any URL starting with &#34;/cyberworld/map/&#34; or &#34;/tmp/&#34;:</p>

<pre>  User-agent: *
  Disallow: /cyberworld/map/ # This is an infinite virtual URL space
  Disallow: /tmp/ # these will soon disappear</pre>

<p>This example &#34;/robots.txt&#34; file specifies that no robots should visit any URL starting with &#34;/cyberworld/map/&#34;, except the robot called &#34;cybermapper&#34;:</p>

<pre>  User-agent: *
  Disallow: /cyberworld/map/ # This is an infinite virtual URL space

  # Cybermapper knows where to go.
  User-agent: cybermapper
  Disallow:</pre>

<p>This example indicates that no robots should visit this site further:</p>

<pre>  # go away
  User-agent: *
  Disallow: /</pre>

<p>This is an example of a malformed robots.txt file.</p>

<pre>  # robots.txt for ancientcastle.example.com
  # I&#39;ve locked myself away.
  User-agent: *
  Disallow: /
  # The castle is your home now, so you can go anywhere you like.
  User-agent: Belle
  Disallow: /west-wing/ # except the west wing!
  # It&#39;s good to be the Prince...
  User-agent: Beast
  Disallow:</pre>

<p>This file is missing the required blank lines between records. However, the intention is clear.</p>

<h1><a class='u' href='#___top' title='click to go to top of document'
name="SEE_ALSO"
>SEE ALSO</a></h1>

<p><a href="../LWP/RobotUA.html" class="podlinkpod"
>LWP::RobotUA</a>, <a href="../WWW/RobotRules/AnyDBM_File.html" class="podlinkpod"
>WWW::RobotRules::AnyDBM_File</a></p>

<h1><a class='u' href='#___top' title='click to go to top of document'
name="COPYRIGHT"
>COPYRIGHT</a></h1>

<pre>  Copyright 1995-2009, Gisle Aas
  Copyright 1995, Martijn Koster</pre>

<p>This library is free software; you can redistribute it and/or modify it under the same terms as Perl itself.</p>
<p class="backlinkbottom"><b><a name="___bottom" href="../index.html" title="All Documents">&lt;&lt;</a></b></p>

<!-- end doc -->

</body></html>
