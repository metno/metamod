<html><head><title>Metamod::UploadHelper</title>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1" >
<link rel="stylesheet" title="blkbluw" type="text/css" href="../_blkbluw.css" media="all" >
<link rel="alternate stylesheet" title="blkmagw" type="text/css" href="../_blkmagw.css" media="all" >
<link rel="alternate stylesheet" title="blkcynw" type="text/css" href="../_blkcynw.css" media="all" >
<link rel="alternate stylesheet" title="whtprpk" type="text/css" href="../_whtprpk.css" media="all" >
<link rel="alternate stylesheet" title="whtnavk" type="text/css" href="../_whtnavk.css" media="all" >
<link rel="alternate stylesheet" title="grygrnk" type="text/css" href="../_grygrnk.css" media="all" >
<link rel="alternate stylesheet" title="whtgrng" type="text/css" href="../_whtgrng.css" media="all" >
<link rel="alternate stylesheet" title="blkgrng" type="text/css" href="../_blkgrng.css" media="all" >
<link rel="alternate stylesheet" title="grygrnw" type="text/css" href="../_grygrnw.css" media="all" >
<link rel="alternate stylesheet" title="blkbluw" type="text/css" href="../_blkbluw.css" media="all" >
<link rel="alternate stylesheet" title="whtpurk" type="text/css" href="../_whtpurk.css" media="all" >
<link rel="alternate stylesheet" title="whtgrng" type="text/css" href="../_whtgrng.css" media="all" >
<link rel="alternate stylesheet" title="grygrnw" type="text/css" href="../_grygrnw.css" media="all" >

<script type="text/javascript" src="../_podly.js"></script>

</head>
<body class='pod'>
<!--
  generated by Pod::Simple::HTML v3.16,
  using Pod::Simple::PullParser v3.16,
  under Perl v5.014002 at Tue May 27 09:39:30 2014 GMT.

 If you want to change this HTML document, you probably shouldn't do that
   by changing it directly.  Instead, see about changing the calling options
   to Pod::Simple::HTML, and/or subclassing Pod::Simple::HTML,
   then reconverting this document from the Pod source.
   When in doubt, email the author of Pod::Simple::HTML for advice.
   See 'perldoc Pod::Simple::HTML' for more info.

-->

<!-- start doc -->
<p class="backlinktop"><b><a name="___top" href="../index.html" accesskey="1" title="All Documents">&lt;&lt;</a></b></p>

<div class='indexgroup'>
<ul   class='indexList indexList1'>
  <li class='indexItem indexItem1'><a href='#NAME'>NAME</a>
  <li class='indexItem indexItem1'><a href='#DESCRIPTION'>DESCRIPTION</a>
  <li class='indexItem indexItem1'><a href='#USAGE'>USAGE</a>
  <li class='indexItem indexItem1'><a href='#METHODS'>METHODS</a>
  <ul   class='indexList indexList2'>
    <li class='indexItem indexItem2'><a href='#read_ftp_events'>read_ftp_events</a>
    <li class='indexItem indexItem2'><a href='#ftp_process_hour'>ftp_process_hour</a>
    <li class='indexItem indexItem2'><a href='#process_upload'>process_upload</a>
    <li class='indexItem indexItem2'><a href='#process_files'>process_files</a>
    <ul   class='indexList indexList3'>
      <li class='indexItem indexItem3'><a href='#Arguments'>Arguments</a>
    </ul>
    <li class='indexItem indexItem2'><a href='#get_dataset_institution'>get_dataset_institution</a>
    <li class='indexItem indexItem2'><a href='#shcommand_scalar'>shcommand_scalar</a>
    <li class='indexItem indexItem2'><a href='#shcommand_array'>shcommand_array</a>
    <li class='indexItem indexItem2'><a href='#move_to_problemdir'>move_to_problemdir</a>
    <li class='indexItem indexItem2'><a href='#purge_current_directory'>purge_current_directory</a>
    <li class='indexItem indexItem2'><a href='#%24self-%3Egunzip_file(_%24filename_)'>$self-&#62;gunzip_file( $filename )</a>
    <li class='indexItem indexItem2'><a href='#%24tar_orignames_%3D_%24self-%3Evalidate_tar_components(%24newpath%2C_%24uploadname%2C_%24dataset_name)'>$tar_orignames = $self-&#62;validate_tar_components($newpath, $uploadname, $dataset_name)</a>
    <li class='indexItem indexItem2'><a href='#%24errors_%3D_%24self-%3Eunpack_tar_archive(%24newpath%2C_%24uploadname%2C_%24work_expand%2C_%24work_flat)%3B'>$errors = $self-&#62;unpack_tar_archive($newpath, $uploadname, $work_expand, $work_flat);</a>
  </ul>
  <li class='indexItem indexItem1'><a href='#%24self-%3Eget_basenames(_%24arrayref_)'>$self-&#62;get_basenames( $arrayref )</a>
  <ul   class='indexList indexList2'>
    <li class='indexItem indexItem2'><a href='#%24upload_helper-%3Eclean_up_repository()'>$upload_helper-&#62;clean_up_repository()</a>
  </ul>
  <li class='indexItem indexItem1'><a href='#DETAILED_OPERATION'>DETAILED OPERATION</a>
  <ul   class='indexList indexList2'>
    <li class='indexItem indexItem2'><a href='#FTP_uploads%3A'>FTP uploads:</a>
    <li class='indexItem indexItem2'><a href='#HTTP_uploads%3A'>HTTP uploads:</a>
    <li class='indexItem indexItem2'><a href='#Overall_operation%3A'>Overall operation:</a>
  </ul>
  <li class='indexItem indexItem1'><a href='#ERROR_HANDLING'>ERROR HANDLING</a>
  <li class='indexItem indexItem1'><a href='#FTP_events_config_file'>FTP events config file</a>
  <li class='indexItem indexItem1'><a href='#SEE_ALSO'>SEE ALSO</a>
  <li class='indexItem indexItem1'><a href='#LICENSE'>LICENSE</a>
</ul>
</div>

<h1><a class='u' href='#___top' title='click to go to top of document'
name="NAME"
>NAME</a></h1>

<p>Metamod::UploadHelper - Helper module for processing file uploads</p>

<h1><a class='u' href='#___top' title='click to go to top of document'
name="DESCRIPTION"
>DESCRIPTION</a></h1>

<p>Monitor file uploads from data providers.
Start digest_nc() on uploaded files.</p>

<h1><a class='u' href='#___top' title='click to go to top of document'
name="USAGE"
>USAGE</a></h1>

<p>Files are either uploaded to an FTP area,
or interactively to an HTTP area using the web interface.
The top level directory pathes for these two areas are given by FIXME [was: the global variables $ftp_dir_path and $upload_dir_path].</p>

<h1><a class='u' href='#___top' title='click to go to top of document'
name="METHODS"
>METHODS</a></h1>

<h2><a class='u' href='#___top' title='click to go to top of document'
name="read_ftp_events"
>read_ftp_events</a></h2>

<p>Load the content of the ftp_events file into a hash.</p>

<h2><a class='u' href='#___top' title='click to go to top of document'
name="ftp_process_hour"
>ftp_process_hour</a></h2>

<p>Check the FTP upload area.</p>

<p>For all datasets scheduled to be processed at the current hour,
check if the newest file in the dataset have large enough age.
If so,
process the files in that dataset.</p>

<h2><a class='u' href='#___top' title='click to go to top of document'
name="process_upload"
>process_upload</a></h2>

<p>Not sure what this does except call process_files() ...</p>

<h2><a class='u' href='#___top' title='click to go to top of document'
name="process_files"
>process_files</a></h2>

<p>Process uploaded files for one dataset from either the FTP or web area.
Names of the uploaded files are found in the global %files_to_process hash.</p>

<p>This routine may also be used to test a file against the repository requirements.
Then,
a dataset for the file need not exist.</p>

<p>Uploaded files are either single files or archives (tar).
Archives are expanded and one archive file will produce many expanded files.
Both single files and archives can be compressed (gzip).
All such files are uncompressed.
The uncompressed expanded files are either netCDF (*.nc) or CDL (*.cdl).
CDL files are converted to netCDF.</p>

<h3><a class='u' href='#___top' title='click to go to top of document'
name="Arguments"
>Arguments</a></h3>

<pre>    $dataset_name     - Name of the dataset
    $ftp_or_web       - =&#39;FTP&#39; if the files are uploaded through FTP,
                        =&#39;WEB&#39; if files are uploaded through the web application.
                        =&#39;TAF&#39; if the file is uploaded just for testing.
    $datestring       - Date/time of the last uploaded file as &#34;YYYY-MM-DD HH:MM&#34;</pre>

<h2><a class='u' href='#___top' title='click to go to top of document'
name="get_dataset_institution"
>get_dataset_institution</a></h2>

<p>Initialize hash connecting each dataset to a reference to a hash with the following elements:</p>

<dl>
<dt><a name="institution"
>institution</a></dt>

<dd>
<p>Name of institution as found in an &#60;heading&#62; element within the webrun/u1 file for the user that owns the dataset.</p>

<dt><a name="email"
>email</a></dt>

<dd>
<p>The owners E-mail address</p>

<dt><a name="name"
>name</a></dt>

<dd>
<p>The owners name</p>

<dt><a name="key"
>key</a></dt>

<dd>
<p>The directory key</p>
</dd>
</dl>

<p>If found, extra elements are included:</p>

<dl>
<dt><a name="location"
>location</a></dt>

<dd>
<p>Location</p>

<dt><a name="catalog"
>catalog</a></dt>

<dd>
<p>Catalog</p>

<dt><a name="wmsurl"
>wmsurl</a></dt>

<dd>
<p>URL to WMS</p>
</dd>
</dl>

<p>The last elements are taken from the line:</p>

<pre>  &#60;dir ... location=&#34;...&#34; catalog=&#34;...&#34; wmsurl=&#34;...&#34;/&#62;)</pre>

<h2><a class='u' href='#___top' title='click to go to top of document'
name="shcommand_scalar"
>shcommand_scalar</a></h2>

<p>Run command via shell (in most cases needlessly as should be Perl modules instead)</p>

<h2><a class='u' href='#___top' title='click to go to top of document'
name="shcommand_array"
>shcommand_array</a></h2>

<p>Run command via shell (in most cases needlessly as should be Perl modules instead)</p>

<p>Returns output split into array on newlines (did that really deserve a copypasted method?)</p>

<h2><a class='u' href='#___top' title='click to go to top of document'
name="move_to_problemdir"
>move_to_problemdir</a></h2>

<p>Move away file, log error</p>

<h2><a class='u' href='#___top' title='click to go to top of document'
name="purge_current_directory"
>purge_current_directory</a></h2>

<p>Delete a bunch of files</p>

<h2><a class='u' href='#___top' title='click to go to top of document'
name="$self-&#62;gunzip_file(_$filename_)"
>$self-&#62;gunzip_file( $filename )</a></h2>

<p>Unzip gzipped file, returning basename minus extension (or .tar if .tgz)</p>

<h2><a class='u' href='#___top' title='click to go to top of document'
name="$tar_orignames_=_$self-&#62;validate_tar_components($newpath,_$uploadname,_$dataset_name)"
>$tar_orignames = $self-&#62;validate_tar_components($newpath, $uploadname, $dataset_name)</a></h2>

<p>extract filenames in tarball</p>

<h2><a class='u' href='#___top' title='click to go to top of document'
name="$errors_=_$self-&#62;unpack_tar_archive($newpath,_$uploadname,_$work_expand,_$work_flat);"
>$errors = $self-&#62;unpack_tar_archive($newpath, $uploadname, $work_expand, $work_flat);</a></h2>

<p>Expand the tar file onto the $work_expand directory. Move all expanded files to the $work_flat directory, which will not contain any subdirectories. Check that no duplicate file names arise.</p>

<h1><a class='u' href='#___top' title='click to go to top of document'
name="$self-&#62;get_basenames(_$arrayref_)"
>$self-&#62;get_basenames( $arrayref )</a></h1>

<p>Apparently strips paths from a list of filenames</p>

<h2><a class='u' href='#___top' title='click to go to top of document'
name="$upload_helper-&#62;clean_up_repository()"
>$upload_helper-&#62;clean_up_repository()</a></h2>

<p>Delete files in repo (only FTP?)</p>

<p>This method is only called from ftp_monitor...</p>

<h1><a class='u' href='#___top' title='click to go to top of document'
name="DETAILED_OPERATION"
>DETAILED OPERATION</a></h1>

<h2><a class='u' href='#___top' title='click to go to top of document'
name="FTP_uploads:"
>FTP uploads:</a></h2>

<p>Uploads to the FTP area are only done by data providers having an agreement with the data repository authority to do so. This agreement designates which datasets the data provider will upload data to. Typically, such agreements are made for operational data uploaded by automatic processes at the data provider site. The names of the datasets covered by such agreements are found in the text file [==WEBRUN_DIRECTORY==]/ftp_events described below.</p>

<p>The script ftp_monitor.pl will search for files at any directory level beneath the $ftp_dir_path directory. Any file that have a basename matching glob pattern &#39;&#60;dataset_name&#62;_*&#39; (where &#60;dataset_name&#62; is the name of the dataset) will be treated as containing a possible addition to that dataset.</p>

<h2><a class='u' href='#___top' title='click to go to top of document'
name="HTTP_uploads:"
>HTTP uploads:</a></h2>

<p>Files uploaded via the web interface are processed with the upload_monitor.pl script. Normally running as a daemon, this starts a worker which waits for events in the job queue.</p>

<p>The directory structure of the HTTP upload area mirrors the directory structure of the final data repository (the $opendap_directory defined below). Thus, any HTTP-uploaded file will end up in a directory where both the institution acronym and the dataset name are found in the directory path. Even so, all file names are required to match the &#39;&#60;dataset_name&#62;_*&#39; pattern (this requirement is enforced by the web interface).</p>

<h2><a class='u' href='#___top' title='click to go to top of document'
name="Overall_operation:"
>Overall operation:</a></h2>

<p>The ftp_events file contains, for each of the datasets, the hours at which the FTP area should be checked for additions. The HTTP area will not be checked &#226;&#128;&#147; now all uploads must register a job in the queue system.</p>

<p>In order to avoid processing of incomplete files, the age of any file has to be above a threshold. This threshold is given in the ftp_events file for files uploaded with FTP, and may vary between datasets. For files uploaded with HTTP, this threshold is a configurable constant ($upload_age_threshold).</p>

<p>Uploaded files are either individual netCDF files (or CDL files), or they are archive files (tar) containing several netCDF/CDL files. Both file types may be gzip compressed. The data provider may upload several files for the same dataset within a short period of time. The digest_nc.pl script will work best if it can, during one invocation, digest all the files uploaded during such a period. To achieve this, the script will not process a file if any other file are found for the same dataset that have not reached the age prescribed by the threshold.</p>

<p>When a new set of files for a given dataset is found to be ready for processing (either from the FTP area or from the HTTP area), the file names are sent to the process_files subroutine.</p>

<p>The process_files subroutine will copy the files to the $work_expand directory where any archive files are expanded. An archive file may contain a directory tree. All files in a directory tree (and all the other files in the $work_expand directory) are copied to another directory, $work_flat. This directory has a flat structure (no subdirectories). A name collision arising from files with same basename but from different parts of a directory tree, is considered an error.</p>

<p>Any CDL file now found in the $work_flat directory is converted to netCDF. The set of uncompressed netCDF-files that now populate the $work_flat directory, is sent to the digest_nc.pl script for checking.</p>

<h1><a class='u' href='#___top' title='click to go to top of document'
name="ERROR_HANDLING"
>ERROR HANDLING</a></h1>

<p>Various errors may arise during this file processing operation. The errors are divided into four different categories:</p>

<p>1. Errors arising from external system environment. Such errors will usually not occur. They will only arise if system resources are exhausted, or if anything happens to the file system (like permission changes on important files and directories). If any such error arise, the script will die and an abortion error message will be recorded in the system error log.</p>

<p>2. Internal system errors. These errors are mainly caused by failing shell commands (file, tar, gunzip etc.). They may also arise when any inconsistency are found that may indicate bugs in the script. The script will continue, but the processing of the offending uploaded file will be discontinued. The file will be moved to the <em>$problem_dir_path</em> directory and an error message will be recorded in the system error log. In addition, the user will be notified about an internal system error that prohibited processing of the file. These errors may be caused by uploaded files that are corrupted, or not of the expected format. (Note to myself: In that case the error category should be changed to category 3 below).</p>

<p>3. User errors that makes furher prosessing of an uploaded file impossible. The file will be moved to the $problem_dir_path directory and an error message will be recorded in the system error log. In addition, the user will be notified with an indication of the nature of the error.</p>

<p>4. Other user errors. These are mainly caused by non-complience with the requirements found in the <em>conf_digest_nc.xml</em> file. All such errors are conveyed to the user through the <em>nc_usererrors.out</em> file. A summary of this file is constructed in the form of a self-explaining HTML file (using the print_usererrors.pl script).</p>

<p>All uploaded files that were processed with no errors, or with only category 4 errors, are deleted after the expanded version of the files are copied to the data repository. The status of the files are recorded in the appropriate file in the u1 subdirectory of the $webrun_directory directory.</p>

<p>In the <em>$problem_dir_path</em> directory the files are renamed according to the following scheme: A 6 digit number, <i>DDNNNN</i>, are constructed where <i>DD</i> is the day number in the month and <i>NNNN</i> is starting on 0001 each new day, and increments with 1 for each file copied to the directory. The new file name will be:</p>

<pre>   DDNNNN_&#60;basename&#62;</pre>

<p>where &#60;basename&#62; is the basename of the uploaded file name.</p>

<p>Files older than a prescribed number of days will be deleted from the $problem_dir_path directory.</p>

<h1><a class='u' href='#___top' title='click to go to top of document'
name="FTP_events_config_file"
>FTP events config file</a></h1>

<p>The FTP events config file regulates which datasets are uploaded through FTP, and how often this script will check for new files for these datasets.</p>

<p>This is a text file which must contain lines of the following format:</p>

<pre>   dataset_name wait_minutes days_to_keep_files hour1 hour2 hour3 ...</pre>

<dl>
<dt><a name="wait_minutes"
>wait_minutes</a></dt>

<dd>
<p>The minimum age of a new ftp file. If a file has less age than this value, the file is left for later processing.</p>

<dt><a name="days_to_keep_files"
>days_to_keep_files</a></dt>

<dd>
<p>Number of days where the files are to remain unchanged on the repository. When this period expires, the files will be deleted and substituted with files containing only metadata. This is done in sub &#39;clean_up_repository&#39;. If this number == 0, the files are kept indefinitely.</p>

<dt><a name="hourN"
>hourN</a></dt>

<dd>
<p>These numbers (0-23) represents the times during a day where checking for new files take place.</p>
</dd>
</dl>

<p>For each hourN, a hash key is constructed as &#34;dataset_name hourN&#34; and the corresponding value is set to wait_minutes.</p>

<h1><a class='u' href='#___top' title='click to go to top of document'
name="SEE_ALSO"
>SEE ALSO</a></h1>

<p><a href="../MetNo/NcDigest.html" class="podlinkpod"
>MetNo::NcDigest</a></p>

<h1><a class='u' href='#___top' title='click to go to top of document'
name="LICENSE"
>LICENSE</a></h1>

<p>GPLv2 <a href="http://www.gnu.org/licenses/gpl-2.0.html" class="podlinkurl"
>http://www.gnu.org/licenses/gpl-2.0.html</a></p>
<p class="backlinkbottom"><b><a name="___bottom" href="../index.html" title="All Documents">&lt;&lt;</a></b></p>

<!-- end doc -->

</body></html>
