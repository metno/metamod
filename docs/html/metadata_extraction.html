<html><head><title>Metadata extraction</title>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1" >
<link rel="stylesheet" type="text/css" title="pod_stylesheet" href="mmdocs.css?view=co">

</head>
<body class='pod'>
<!--
  generated by Metamod::Pod::Simple::HTML v,
  using Pod::Simple::PullParser v3.16,
  under Perl v5.014002 at Thu Feb 28 09:40:10 2013 GMT.

 If you want to change this HTML document, you probably shouldn't do that
   by changing it directly.  Instead, see about changing the calling options
   to Metamod::Pod::Simple::HTML, and/or subclassing Metamod::Pod::Simple::HTML,
   then reconverting this document from the Pod source.
   When in doubt, email the author of Metamod::Pod::Simple::HTML for advice.
   See 'perldoc Metamod::Pod::Simple::HTML' for more info.

-->

<!-- start doc -->
<a name='___top' class='dummyTopAnchor' ></a>

<div class='indexgroup'>
<ul   class='indexList indexList1'>
  <li class='indexItem indexItem1'><a href='#Metadata_extraction'>Metadata extraction</a>
  <ul   class='indexList indexList2'>
    <li class='indexItem indexItem2'><a href='#Upload_via_web_interface'>Upload via web interface</a>
    <li class='indexItem indexItem2'><a href='#Upload_via_FTP'>Upload via FTP</a>
    <li class='indexItem indexItem2'><a href='#Metadata_extraction_from_external_repository'>Metadata extraction from external repository</a>
  </ul>
</ul>
</div>

<h1><a class='u' href='#___top' title='click to go to top of document'
name="Metadata_extraction"
>Metadata extraction</a></h1>

<p>Back to <a href="./index.html?view=co" class="podlinkpod"
>Index</a></p>

<p>METAMOD supports extracting metadata from netCDF files.
The metadata extraction is triggered in three different ways.</p>

<ul>
<li>A user uploads a data file or tar archive via the web interface.</li>

<li>Data files are upload via a FTP to a folder that is monitored by METAMOD.</li>

<li>Data files are placed in a folder available to METAMOD and then a HTTP request is sent to METAMOD to trigger the metadata extraction.</li>
</ul>

<p>Which method is used for triggering uploads are decided by the <code>EXTERNAL_REPOSITORY</code> configuration variables.</p>

<p>If <code>EXTERNAL_REPOSTORY</code> is set to <code>false</code> the two first methods can be used.
In this case METAMOD is also responsible for moving the data files to the data repository in addition to extracting the metadata.</p>

<p>If <code>EXTERNAL_REPOSITORY</code> is set to <code>true</code> the second method is used.
In this case METAMOD will <b>not</b> move the data files to the data repository.
The data repository is then some other system responsibility.</p>

<h2><a class='u' href='#___top' title='click to go to top of document'
name="Upload_via_web_interface"
>Upload via web interface</a></h2>

<p>When a user uploads a new data file to the system using the web interface the following steps are executed.</p>

<ul>
<li>The web application performs some validation of the file and the name of the dataset and if all validations are ok it stores the file in the directory specified in the configuration variable <code>UPLOAD_DIRECTORY</code>.
<p>The web application also inserts an &#34;Upload&#34; job into the queue to tell the backend system that a new file is available.</p>

<p>At this point the web application sends a reply back to the client.</p>
</li>

<li>Now that there is a now job in the queue a <b>worker process</b> will start to execute the job as soon as a worker process is available.
<p>For the Upload job,
<code>Metamod::Queue::Worker::Upload</code> and <code>Metamod::Queue::Job::Upload</code> is used to perform the job.
The reason for having two modules for doing this work is that Worker module is specific the queue system used,
while the Job module is independent of the actual queue system.
This makes it easier to switch from one queue system to another at a later date.</p>
</li>

<li>When <code>Metamod::Queue::Job::Upload</code> has started to execute the job it will leave the heavy lifting to <code>Metamod::UploadHelper</code>.
UploadHelper will do the following:
<ul>
<li>Create a directory structure where it can put temporary files while it is working.</li>

<li>If the upload file is a gz,
tar.gz or tar file it will extract the contents of archive and then continue to work on the contents of the archive.</li>

<li>If any of the uploaded files are CDL files it will try to convert the files to netCDF files.</li>

<li>When all the files are ready for processing it will use <code>MetNo::DigestNc</code> to extract the metadata from the netCDF files.
This processing in done twice.
Once for all the uploaded at the same time to extract the level 1 metadata for the dataset.
Then DigestNc is used again for each file to extract the level 2 metadata for that file.
<p>DigestNc use <code>Metamod::Dataset</code> for reading and writing metadata so all data is stored both in the XML metadata repository and the index database.</p>
</li>

<li>After the metadata has been extracted the data files are moved to the data repository.</li>

<li>Depending on how the processing of the files went the UploadHelper will notify the user on the result of the upload via email and also write the result of the upload to the User database.
<p>In case of any error in the processing of the file the UploadHelper will generate a HTML file with a description of all the errors.</p>
</li>
</ul>
</li>
</ul>

<h2><a class='u' href='#___top' title='click to go to top of document'
name="Upload_via_FTP"
>Upload via FTP</a></h2>

<p>Upload via FTP is very similar to upload via web except for the first steps that triggers the metadata extraction.</p>

<p>Instead of using the queue system the FTP upload is triggered by the script upload/scripts/ftp_monitor.pl.
This script supports several different modes but the most relevant here is the daemon mode where it is running like a CRON-like job.</p>

<p>When running as a CRON-like job the script will wake up at regular intervals and start looking for files in the <code>FTP_UPLOAD_DIRECTORY</code>.</p>

<p>When looking for files in the ftp upload directory it will not process all files that it finds.
What files to process is specificed in the <code>ftp_events</code> file in the <code>WEBRUN_DIRECTORY</code>.</p>

<p>When a file is found in the FTP directory the metadata extraction continous in the same way for web uploads using the <code>Metamod::UploadHelper</code>.</p>

<h2><a class='u' href='#___top' title='click to go to top of document'
name="Metadata_extraction_from_external_repository"
>Metadata extraction from external repository</a></h2>

<p>Metadata extraction when data is stored in a external repository works differently than the two other methods.</p>

<p>In this case the process is as follows:</p>

<ul>
<li>A process external to METAMOD places one or more data files in a directory that is available from the METAMOD instance.</li>

<li>The external process notifies METAMOD that new files are ready for metadata extraction by sending a HTTP GET message to /upload/newfiles with the correct parameters.</li>

<li>The METAMOD web application will receive the HTTP request and then start the script <code>upload/script/upload_index.pl</code>.</li>

<li>The upload_index.pl script will then follow the same steps as UploadHelper for extracting the metadata.
The only difference is that it will <b>not</b> move the data files into a data repository.</li>
</ul>

<!-- end doc -->

</body></html>
